{"cells":[{"cell_type":"markdown","id":"69d9f01e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"78c4d45d1f56d116ef9ef6a4e0a1f0e7","grade":false,"grade_id":"cell-4dad29f85abd546f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"69d9f01e"},"source":["# Assignment 2a: Language Modelling via N-grams (5 Marks)\n","\n","## Due: March 17, 2022\n","\n","Welcome to the first part of Assignment 2. In this assignment you will implement N-gram language models as discussed in lecture 3 and 4 of the course. We will be working with the [WikiText-2 dataset](https://paperswithcode.com/dataset/wikitext-2) which consists of about 100 million tokens collected from Good and Featured articles from wikipedia."]},{"cell_type":"code","execution_count":3,"id":"b3d07e98","metadata":{"id":"b3d07e98","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646944614740,"user_tz":-330,"elapsed":24338,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"556656b4-f30d-4771-be4c-4fc056629a1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    data_dir = \"gdrive/MyDrive/PlakshaNLP/Assignment2a/data/wikitext-2\"\n","except:\n","    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment2a/data/wikitext-2/\""]},{"cell_type":"code","execution_count":4,"id":"178e3820","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"208a3686cd75f206390a99b912d7d829","grade":false,"grade_id":"cell-21f93cdd721bf4ba","locked":true,"schema_version":3,"solution":false,"task":false},"id":"178e3820","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646944622157,"user_tz":-330,"elapsed":7422,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"e966b07a-d765-4b29-8ba3-3996f0830100"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}],"source":["# Install required libraries\n","!pip install numpy\n","!pip install pandas\n","!pip install nltk"]},{"cell_type":"code","execution_count":5,"id":"b36ca8cc","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e1570ffda89cc3709398a7df2272bd72","grade":false,"grade_id":"cell-5279dd8d57f286fd","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b36ca8cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646944624080,"user_tz":-330,"elapsed":1930,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"88e065bb-2331-4d62-fb62-5bacb593becf"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# We start by importing libraries that we will be making use of in the assignment.\n","import string\n","from collections import defaultdict\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download(\"punkt\")\n","nltk.download('stopwords')"]},{"cell_type":"markdown","id":"0a28b578","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1555fb9d5057e572d16bc628a97cd370","grade":false,"grade_id":"cell-cf1fc2b2db15dff0","locked":true,"schema_version":3,"solution":false,"task":false},"id":"0a28b578"},"source":["We start by loading Wiki-Text dataset into memory. It consists of 3 text files one each for train, validation and test, consisting raw text data corresponding to different wikipedia articles."]},{"cell_type":"code","execution_count":6,"id":"b5005fc6","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0a348a0412559dcb0d1f176e9e25e387","grade":false,"grade_id":"cell-43171f704b366858","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b5005fc6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646944625915,"user_tz":-330,"elapsed":1840,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"f0670c19-c4ff-4170-b272-98bbc68e5e82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets Loaded!\n","Number of characters in train corpus: 10780437\n","Number of characters in validation corpus: 1120192\n","Number of characters in test corpus: 1255018\n"]}],"source":["# Load train data\n","with open(f\"{data_dir}/wiki.train.tokens\") as f:\n","    wiki_train = f.read()\n","\n","# Load validation data\n","with open(f\"{data_dir}/wiki.valid.tokens\") as f:\n","    wiki_valid = f.read()\n","\n","# Load Test data\n","with open(f\"{data_dir}/wiki.test.tokens\") as f:\n","    wiki_test = f.read()\n","    \n","print(f\"Datasets Loaded!\")\n","print(f\"Number of characters in train corpus: {len(wiki_train)}\")\n","print(f\"Number of characters in validation corpus: {len(wiki_valid)}\")\n","print(f\"Number of characters in test corpus: {len(wiki_test)}\")"]},{"cell_type":"code","execution_count":7,"id":"83462af5","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8f6ad0493e137468e30f2c2c74258e05","grade":false,"grade_id":"cell-8e48dba7173b8e38","locked":true,"schema_version":3,"solution":false,"task":false},"id":"83462af5","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1646944625915,"user_tz":-330,"elapsed":25,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"9a29320b-41a4-4731-fcca-2a468cf45db2"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,'"]},"metadata":{},"execution_count":7}],"source":["# Lets print first 100 characters of the train_corpus\n","wiki_train[:100]"]},{"cell_type":"markdown","id":"f67af028","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1346493e9471b11fe669e297f907dfba","grade":false,"grade_id":"cell-d0144eb7bcea588b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"f67af028"},"source":["## Task 1: Fit n-gram probabilities (2 Marks)\n","\n","N-gram language models are trained by estimating the joint and conditional probabilities of all the n-grams from the training corpus. Using these probabilities we can then sample next sequence of tokens given the context or compute the probability of a complete piece of text."]},{"cell_type":"markdown","id":"3a084a19","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f00167d218520c4c12feeb5b1302d17b","grade":false,"grade_id":"cell-b81f32aabd61b562","locked":true,"schema_version":3,"solution":false,"task":false},"id":"3a084a19"},"source":["### Task 1.1: Unigram Probabilities (0.75 Mark)\n","\n","We will start by estimating unigram probabilities from the corpus, which can be simply done by calculating the normalized frequency of each token in the corpus. Implement the `get_unigram_probs` function below that does that"]},{"cell_type":"code","execution_count":22,"id":"155ef81c","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"16977f9ce3a0d0fcc89d5144a68d2c8a","grade":false,"grade_id":"cell-9894d67d3e54861b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"155ef81c","executionInfo":{"status":"ok","timestamp":1646945871592,"user_tz":-330,"elapsed":512,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["from collections import defaultdict\n","from collections import Counter\n","\n","def get_unigram_probs(corpus):\n","    \"\"\"\n","    Estimates the probability of each unigram in the text by calculating\n","    normalized frequency of each unigram (word).\n","    \n","    Inputs:\n","        - corpus (str): A python string consisting of a piece of text\n","    \n","    Returns:\n","        - unigram_probs (dict): A python dictionary with unigrams (words) as keys and \n","                                their frequencies as values\n","                                \n","    Example Input: \"I am Sam . Sam I am . I do not like green eggs and ham .\"\n","    Expected Output: {'I': 0.17647058823529413,\n","             'am': 0.11764705882352941,\n","             'Sam': 0.11764705882352941,\n","             '.': 0.17647058823529413,\n","             'do': 0.058823529411764705,\n","             'not': 0.058823529411764705,\n","             'like': 0.058823529411764705,\n","             'green': 0.058823529411764705,\n","             'eggs': 0.058823529411764705,\n","             'and': 0.058823529411764705,\n","             'ham': 0.058823529411764705}\n","    \n","    Note: To break the text into tokens you can just use corpus.split() this time,\n","    Wiki corpus is already tokenized so just using split() will also work fine.\n","    **Do not use `nltk.tokenize.word_tokenize` here**\n","\n","    Hint: `defaultdict` can often make life easier in such type of problems.\n","            https://docs.python.org/3/library/collections.html#collections.defaultdict\n","    \"\"\"\n","    \n","    sentence = corpus.split()\n","    c = Counter(sentence)\n","    unigram_probs = dict(c)\n","    sumval = sum(unigram_probs.values())\n","    # unigram_probs = dict.fromkeys(sentence,0)\n","    # for word in sentence:\n","    #     unigram_probs[word] = sentence.count(word)\n","    \n","    # sumval = sum(unigram_probs.values())\n","    unigram_probs = {k: v / sumval for k, v in unigram_probs.items()}\n","    return unigram_probs"]},{"cell_type":"code","source":["sample_corpus = 'I am Sam . Sam I am . I do not like green eggs and ham .'\n","get_unigram_probs(sample_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uX_xN1bvnLor","executionInfo":{"status":"ok","timestamp":1646945873574,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"c013040d-3b27-41b0-bf3a-77e57e30d9fb"},"id":"uX_xN1bvnLor","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'.': 0.17647058823529413,\n"," 'I': 0.17647058823529413,\n"," 'Sam': 0.11764705882352941,\n"," 'am': 0.11764705882352941,\n"," 'and': 0.058823529411764705,\n"," 'do': 0.058823529411764705,\n"," 'eggs': 0.058823529411764705,\n"," 'green': 0.058823529411764705,\n"," 'ham': 0.058823529411764705,\n"," 'like': 0.058823529411764705,\n"," 'not': 0.058823529411764705}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":24,"id":"b72ac9b0","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f4e5be8dfec5715e4b53b64a2f877067","grade":true,"grade_id":"cell-46656780cd552710","locked":true,"points":0.75,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"b72ac9b0","executionInfo":{"status":"ok","timestamp":1646945876303,"user_tz":-330,"elapsed":483,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"7c78b071-7b59-4eab-95db-ca1685a6c6b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Case 1\n","Input Corpus: I am Sam . Sam I am . I do not like green eggs and ham .\n","Unigram Probabilities: {'I': 0.17647058823529413, 'am': 0.11764705882352941, 'Sam': 0.11764705882352941, '.': 0.17647058823529413, 'do': 0.058823529411764705, 'not': 0.058823529411764705, 'like': 0.058823529411764705, 'green': 0.058823529411764705, 'eggs': 0.058823529411764705, 'and': 0.058823529411764705, 'ham': 0.058823529411764705}\n","Expected Unigram Probabilities: {'I': 0.17647058823529413, 'am': 0.11764705882352941, 'Sam': 0.11764705882352941, '.': 0.17647058823529413, 'do': 0.058823529411764705, 'not': 0.058823529411764705, 'like': 0.058823529411764705, 'green': 0.058823529411764705, 'eggs': 0.058823529411764705, 'and': 0.058823529411764705, 'ham': 0.058823529411764705}\n","Sample Test Case Passed\n","****************************************\n","\n","Running Sample Test Case 2\n","Input Corpus: john likes to watch movies mary likes movies too mary also likes to watch football games\n","Unigram Probabilities: {'john': 0.0625, 'likes': 0.1875, 'to': 0.125, 'watch': 0.125, 'movies': 0.125, 'mary': 0.125, 'too': 0.0625, 'also': 0.0625, 'football': 0.0625, 'games': 0.0625}\n","Expected Unigram Probabilities: {'john': 0.0625, 'likes': 0.1875, 'to': 0.125, 'watch': 0.125, 'movies': 0.125, 'mary': 0.125, 'too': 0.0625, 'also': 0.0625, 'football': 0.0625, 'games': 0.0625}\n","Sample Test Case Passed\n","****************************************\n","\n"]}],"source":["def check_dicts_same(dict1, dict2):\n","    if not (isinstance(dict1, dict) or isinstance(dict1, defaultdict)):\n","        print(\"Your function output is not a dictionary!\")\n","        return False\n","    if len(dict1) != len(dict2):\n","        return False\n","    \n","    for key in dict1:\n","        val1 = dict1[key]\n","        val2 = dict2[key]\n","        if isinstance(val1, float) and isinstance(val2, float):\n","            if not np.allclose(val1, val2, 1e-4):\n","                return False\n","        if val1 != val2:\n","            return False\n","    \n","    return True\n","\n","print(\"Running Sample Test Case 1\")\n","sample_corpus = 'I am Sam . Sam I am . I do not like green eggs and ham .'\n","exp_unigram_probs = {'I': 0.17647058823529413,\n","             'am': 0.11764705882352941,\n","             'Sam': 0.11764705882352941,\n","             '.': 0.17647058823529413,\n","             'do': 0.058823529411764705,\n","             'not': 0.058823529411764705,\n","             'like': 0.058823529411764705,\n","             'green': 0.058823529411764705,\n","             'eggs': 0.058823529411764705,\n","             'and': 0.058823529411764705,\n","             'ham': 0.058823529411764705}\n","\n","output_unigram_probs = get_unigram_probs(sample_corpus)\n","print(f\"Input Corpus: {sample_corpus}\")\n","print(f\"Unigram Probabilities: {output_unigram_probs}\")\n","print(f\"Expected Unigram Probabilities: {output_unigram_probs}\")\n","\n","assert check_dicts_same(exp_unigram_probs, output_unigram_probs)\n","print(\"Sample Test Case Passed\")\n","print(\"****************************************\\n\")\n","    \n","print(\"Running Sample Test Case 2\")\n","sample_corpus = 'john likes to watch movies mary likes movies too mary also likes to watch football games'\n","exp_unigram_probs = {'john': 0.0625,\n","                     'likes': 0.1875,\n","                     'to': 0.125,\n","                     'watch': 0.125,\n","                     'movies': 0.125,\n","                     'mary': 0.125,\n","                     'too': 0.0625,\n","                     'also': 0.0625,\n","                     'football': 0.0625,\n","                     'games': 0.0625}\n","\n","output_unigram_probs = get_unigram_probs(sample_corpus)\n","print(f\"Input Corpus: {sample_corpus}\")\n","print(f\"Unigram Probabilities: {output_unigram_probs}\")\n","print(f\"Expected Unigram Probabilities: {exp_unigram_probs}\")\n","\n","assert check_dicts_same(output_unigram_probs, exp_unigram_probs)\n","print(\"Sample Test Case Passed\")\n","print(\"****************************************\\n\")\n"]},{"cell_type":"markdown","id":"ca70a674","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"58e3c6b8f1ccd263ddc4b898c937f964","grade":false,"grade_id":"cell-cad2c95328fb6979","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ca70a674"},"source":["Let's get the unigram probabilities from the entire training corpus now"]},{"cell_type":"code","execution_count":25,"id":"36aadd91","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"347a78332953ce36c99023b81435c6f7","grade":false,"grade_id":"cell-10bd3b75407498ee","locked":true,"schema_version":3,"solution":false,"task":false},"id":"36aadd91","executionInfo":{"status":"ok","timestamp":1646945882348,"user_tz":-330,"elapsed":509,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["train_unigram_probs = get_unigram_probs(wiki_train)"]},{"cell_type":"markdown","id":"afd9cc79","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a4a9a5cb0b8fa6d6be678916a13b44e9","grade":false,"grade_id":"cell-e882c4441289799e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"afd9cc79"},"source":["### Task 1.2: N-gram Probabilities (1.25 Marks)\n","\n","Now let's estimate the probabilities for N-grams with N > 1. For N-grams instead of storing joint probability of each N-gram in the corpus i.e. p(w_n-N+1, ...,w_n-1, w_n) we store conditional probabilities p(w_n | w_n-N+1, ...,w_n-1), which can be calculated easily by using the following formula:\n","\n","<img src=\"https://i.ibb.co/mtFcBfh/n-gram.jpg\" alt=\"n-gram\" border=\"0\">\n","\n","Here the term in numerator denotes the number of N-grams with the words w_n-N+1, ...,w_n-1, w_n and the term in denominator denotes the number of (N-1) grams with the words w_n-N+1, ...,w_n-1 .\n","\n","Implement the `get_ngram_cond_probs` function below"]},{"cell_type":"code","execution_count":32,"id":"678d7d85","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"08f54f5cab96950494180af8cf9a469b","grade":false,"grade_id":"cell-d2dd7df0fa9339e9","locked":false,"schema_version":3,"solution":true,"task":false},"id":"678d7d85","executionInfo":{"status":"ok","timestamp":1646946663790,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["def get_ngram_cond_probs(corpus, N = 2):\n","    \"\"\"\n","    Estimates the N-gram conditional probabilities from the corpus.\n","    \n","    Inputs:\n","        - corpus (str): A python string consisting of a piece of text\n","        - N (int) : Value of  N in N-gram\n","        \n","    Returns:\n","        - ngram_cond_probs (dict) : A dictionary with (N-1)-gram tuple as keys and values as dictionaries\n","                                    such that ngram_cond_probs[(w_n-N+1, ...,w_n-1)][w_n]\n","                                    stores the conditional probability p(w_n | w_n-N+1, ...,w_n-1)\n","                                    See the examples below for clarity\n","                                    \n","    Example Input: corpus = 'I am Sam . Sam I am .', N = 2\n","    Expected Output: \n","    {\n","         ('I',): {'am': 1.0},\n","         ('am',): {'Sam': 0.5, '.': 0.5},\n","         ('Sam',): {'.': 0.5, 'I': 0.5},\n","         ('.',): {'Sam': 1.0}\n"," \n","     }\n","     Explanation: 'I' is always followed 'am' in the `corpus`, while 'am' is followed by 'Sam' once and '.' the other time\n","                     hence you see 0.5, 0.5 probabilities for these two tokens.\n","     \n","    Example Input: corpus = 'I am Sam . Sam I am .', N = 3\n","    Expected Output:\n","    {\n","         ('I', 'am'): {'Sam': 0.5, '.': 0.5},\n","         ('am', 'Sam'): {'.': 1.0},\n","         ('Sam', '.'): {'Sam': 1.0},\n","         ('.', 'Sam'): {'I': 1.0},\n","         ('Sam', 'I'): {'am': 1.0}\n","     }\n"," \n","    \"\"\"\n","    \n","    ngram_cond_probs = {}\n","\n","    # This function will give a list of all the N sized words.\n","    # After which we split last word, take out conditional prob, put in dict.\n","\n","    sentence = corpus.split()\n","    biglist = []\n","    for i in range(len(sentence)-N+1):\n","        lis = []\n","        k=0\n","        while k<N:\n","            lis.append(sentence[i+k])\n","            k+=1\n","        biglist.append(lis)\n","    \n","    d = defaultdict(list)\n","    for k in biglist:\n","        d[tuple(k[:N-1])].append(k[N-1])\n","    ngram_cond_probs = {k: get_unigram_probs(' '.join(v)) for k, v in d.items()}\n","    return ngram_cond_probs\n","\n","\n","    # # actually instead, we can make a list where the first N words are common \n","    # # and then we put the function get_unigram_probs on the words after N.\n","    \n","    # new = []\n","    # for i in range(len(biglist)):\n","    #     for j in range(i+1,len(biglist)):\n","    #         if biglist[i][:N-1] == biglist[j][:N-1]:\n","    #             biglist[i].append(biglist[j][N-1])\n","    #     new.append(biglist[i])\n","\n","    # for i in range(len(new)):\n","    #     for j in range(i+1,len(new)):\n","    #         # print(new[i], new[j])\n","    #         if new[i][:N-1] == new[j][:N-1]:\n","    #             new.remove(new[j])\n","\n","    # # return new\n","    # for item in new:\n","    #     ngram_cond_probs[tuple(item[:N-1])] = get_unigram_probs(' '.join(item[N-1:]))\n","    # return ngram_cond_probs\n","    # # smalllist = []\n","    # # for item in biglist:\n","    # #     last = item.pop()\n","    # #     smalllist.append(last)\n","    \n","    # # return biglist, smalllist\n","\n","\n","    "]},{"cell_type":"code","source":["corpus = 'I am Sam . Sam I am .'\n","print(get_ngram_cond_probs('I am Sam . Sam I am .', N = 2))\n","print(get_ngram_cond_probs('I am Sam . Sam I am .', N = 3))"],"metadata":{"id":"qRlbi301F2J8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646946667686,"user_tz":-330,"elapsed":420,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"35ab1f44-5868-46dd-9bd4-83a2c6a8dc81"},"id":"qRlbi301F2J8","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["{('I',): {'am': 1.0}, ('am',): {'Sam': 0.5, '.': 0.5}, ('Sam',): {'.': 0.5, 'I': 0.5}, ('.',): {'Sam': 1.0}}\n","{('I', 'am'): {'Sam': 0.5, '.': 0.5}, ('am', 'Sam'): {'.': 1.0}, ('Sam', '.'): {'Sam': 1.0}, ('.', 'Sam'): {'I': 1.0}, ('Sam', 'I'): {'am': 1.0}}\n"]}]},{"cell_type":"code","execution_count":34,"id":"61e92fd9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"40ad85b65a1e0802cdc32f70411080a2","grade":true,"grade_id":"cell-50b5e23ba10e4996","locked":true,"points":1.25,"schema_version":3,"solution":false,"task":false},"id":"61e92fd9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646946670747,"user_tz":-330,"elapsed":507,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"1b4d16f2-1c22-40de-f0ca-8100a408805b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Case 1\n","Input Corpus: I am Sam . Sam I am ., N = 2\n","N-gram Probabilities: {('I',): {'am': 1.0}, ('am',): {'Sam': 0.5, '.': 0.5}, ('Sam',): {'.': 0.5, 'I': 0.5}, ('.',): {'Sam': 1.0}}\n","Expected N-gram Probabilities: {('I',): {'am': 1.0}, ('am',): {'Sam': 0.5, '.': 0.5}, ('Sam',): {'.': 0.5, 'I': 0.5}, ('.',): {'Sam': 1.0}}\n","Sample Test Case Passed\n","****************************************\n","\n","Running Sample Test Case 2\n","Input Corpus: I am Sam . Sam I am ., N = 3\n","N-gram Probabilities: {('I', 'am'): {'Sam': 0.5, '.': 0.5}, ('am', 'Sam'): {'.': 1.0}, ('Sam', '.'): {'Sam': 1.0}, ('.', 'Sam'): {'I': 1.0}, ('Sam', 'I'): {'am': 1.0}}\n","Expected N-gram Probabilities: {('I', 'am'): {'Sam': 0.5, '.': 0.5}, ('am', 'Sam'): {'.': 1.0}, ('Sam', '.'): {'Sam': 1.0}, ('.', 'Sam'): {'I': 1.0}, ('Sam', 'I'): {'am': 1.0}}\n","Sample Test Case Passed\n","****************************************\n","\n","Running Sample Test Case 3\n","Input Corpus: I am Sam . Sam I am ., N = 4\n","N-gram Probabilities: {('I', 'am', 'Sam'): {'.': 1.0}, ('am', 'Sam', '.'): {'Sam': 1.0}, ('Sam', '.', 'Sam'): {'I': 1.0}, ('.', 'Sam', 'I'): {'am': 1.0}, ('Sam', 'I', 'am'): {'.': 1.0}}\n","Expected N-gram Probabilities: {('I', 'am', 'Sam'): {'.': 1.0}, ('am', 'Sam', '.'): {'Sam': 1.0}, ('Sam', '.', 'Sam'): {'I': 1.0}, ('.', 'Sam', 'I'): {'am': 1.0}, ('Sam', 'I', 'am'): {'.': 1.0}}\n","Sample Test Case Passed\n","****************************************\n","\n"]}],"source":["def check_dicts_same_v2(dict1, dict2):\n","    if not (isinstance(dict1, dict) or isinstance(dict1, defaultdict)):\n","        print(\"Your function output is not a dictionary!\")\n","        return False\n","    \n","    if len(dict1) != len(dict2):\n","        return False\n","        \n","    for key in dict1:\n","        val1 = dict1[key]\n","        val2 = dict2[key]\n","        if isinstance(val1, float) and isinstance(val2, float):\n","            if not np.allclose(val1, val2, 1e-4):\n","                return False\n","        if (isinstance(val1, dict) or isinstance(val1, defaultdict)) \\\n","            and (isinstance(val2, dict) or isinstance(val2, defaultdict)):\n","            if not check_dicts_same_v2(val1, val2):\n","                return False\n","        if val1 != val2:\n","            return False\n","    \n","    return True\n","\n","print(\"Running Sample Test Case 1\")\n","sample_corpus = 'I am Sam . Sam I am .'\n","N = 2\n","exp_ngram_probs =     {\n","         ('I',): {'am': 1.0},\n","         ('am',): {'Sam': 0.5, '.': 0.5},\n","         ('Sam',): {'.': 0.5, 'I': 0.5},\n","         ('.',): {'Sam': 1.0}\n","     }\n","\n","output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n","print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n","print(f\"N-gram Probabilities: {output_ngram_probs}\")\n","print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n","\n","assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n","print(\"Sample Test Case Passed\")\n","print(\"****************************************\\n\")\n","\n","print(\"Running Sample Test Case 2\")\n","sample_corpus = 'I am Sam . Sam I am .'\n","N = 3\n","exp_ngram_probs =         {\n","         ('I', 'am'): {'Sam': 0.5, '.': 0.5},\n","         ('am', 'Sam'): {'.': 1.0},\n","         ('Sam', '.'): {'Sam': 1.0},\n","         ('.', 'Sam'): {'I': 1.0},\n","         ('Sam', 'I'): {'am': 1.0}\n","     }\n","\n","output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n","print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n","print(f\"N-gram Probabilities: {output_ngram_probs}\")\n","print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n","\n","assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n","print(\"Sample Test Case Passed\")\n","print(\"****************************************\\n\")\n","\n","print(\"Running Sample Test Case 3\")\n","sample_corpus = 'I am Sam . Sam I am .'\n","N = 4\n","exp_ngram_probs =  {('I', 'am', 'Sam'): {'.': 1.0},\n","             ('am', 'Sam', '.'): {'Sam': 1.0},\n","             ('Sam', '.', 'Sam'): {'I': 1.0},\n","             ('.', 'Sam', 'I'): {'am': 1.0},\n","             ('Sam', 'I', 'am'): {'.': 1.0}}\n","\n","output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n","print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n","print(f\"N-gram Probabilities: {output_ngram_probs}\")\n","print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n","\n","assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n","print(\"Sample Test Case Passed\")\n","print(\"****************************************\\n\")\n"]},{"cell_type":"markdown","id":"e93dc0a9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"027b0659ee2dc75e70d00cd89cdc3e96","grade":false,"grade_id":"cell-0c43ed56dbe13464","locked":true,"schema_version":3,"solution":false,"task":false},"id":"e93dc0a9"},"source":["We can now generate N-gram probability distributions for our dataset"]},{"cell_type":"code","execution_count":35,"id":"21a315d3","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"61b1117df6d92bed9230d11f967a5eaf","grade":false,"grade_id":"cell-d817f2a967337003","locked":true,"schema_version":3,"solution":false,"task":false},"id":"21a315d3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646946702411,"user_tz":-330,"elapsed":26665,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"c3b41900-c7b6-4f50-e06c-016103456ae0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generating Bigram Distribution\n","Generating Trigram Distribution\n","Generating 4-gram Distribution\n"]}],"source":["print(\"Generating Bigram Distribution\")\n","bigram_prob_dist = get_ngram_cond_probs(wiki_train, N = 2)\n","\n","print(\"Generating Trigram Distribution\")\n","trigram_prob_dist = get_ngram_cond_probs(wiki_train, N = 3)\n","\n","print(\"Generating 4-gram Distribution\")\n","tetragram_prob_dist = get_ngram_cond_probs(wiki_train, N = 4)"]},{"cell_type":"markdown","id":"6b8f4244","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"787553d25794b79bca8e4544ed445f97","grade":false,"grade_id":"cell-2e3e35cb9475d27b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"6b8f4244"},"source":["## Task 2: Evaluating the language models (2.5 Marks)\n","\n","Now that we have generated the N-gram distributions for various Ns we can evaluate the language models both quantitatively using the perplexity metric as well as qualitatively by generating text using these models."]},{"cell_type":"markdown","id":"e53c12d3","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"df3bbf0afd04cd2ceb252099dde2c984","grade":false,"grade_id":"cell-85ffb0083a4e789b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"e53c12d3"},"source":["### Task 2.1: Calculating Perplexity (0.5 Marks)\n","\n","As discussed in Lecture 4, perplexity of a probability distribution is defined as:\n","\n","<img src=\"https://i.ibb.co/swXqFCz/Perplexity.jpg\" alt=\"Perplexity\" border=\"0\">\n","\n","For the purposes of this assignment we will only calculate perplexity of a unigram language model. Finding perplexity of N-gram models for N > 1 requires smoothing which is beyond the scope of this assignment, but interested students can read about it [here](https://web.stanford.edu/~jurafsky/slp3/3.pdf). For now, implement the `get_unigram_perplexity` function below"]},{"cell_type":"code","execution_count":36,"id":"acc9357e","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e02e9c75a23a5f9b9e03d52354d475ea","grade":false,"grade_id":"cell-8c5a8fb5ce9456e5","locked":false,"schema_version":3,"solution":true,"task":false},"id":"acc9357e","executionInfo":{"status":"ok","timestamp":1646946729017,"user_tz":-330,"elapsed":414,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["import math\n","\n","def get_unigram_perplexity(corpus, unigram_probs):\n","    \"\"\"\n","    Calculates the perplexity of a unigram language model on a text corpus\n","    \n","    Inputs:\n","        - corpus (str): A python string consisting of a piece of text    \n","        - unigram_probs (dict): A python dictionary with unigrams (words) as keys and \n","                                their probabilities as values\n","    Returns:\n","        - perplexity (float) : Perplexity of unigram model on corpus\n","    \n","    Note 1: You can assume that each word/unigram that appears in the corpus, we have it's probability in `unigram_probs`\n","    Note 2: Use log to the base 2 for calculating entropy\n","    \"\"\"\n","    \n","    # perplexity = None\n","    \n","    # YOUR CODE HERE\n","    \n","    # raise NotImplementedError()\n","    words = corpus.split()\n","    entropy = 0\n","    for word in words:\n","        entropy += unigram_probs[word] * math.log2(unigram_probs[word])\n","    perplexity = 2**(-1*entropy)\n","    \n","    return perplexity"]},{"cell_type":"code","execution_count":37,"id":"2b4195e8","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f004b8bc9e92500099a95d92a47b177e","grade":true,"grade_id":"cell-407824efd9e6de69","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"2b4195e8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646946731783,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"150828f4-5aad-4b2b-c3a8-70057ebee858"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data Perplexity: 66.87292185710841\n","Expected Training Data Perplexity: 66.87292185710841\n","**********************************\n","\n","Validation Data Perplexity: 73.3487215828768\n","Expected Validation Data Perplexity: 73.3487215828768\n","**********************************\n","\n","Test Data Perplexity: 54.97601260063733\n","Expected Test Data Perplexity: 54.97601260063733\n","**********************************\n","\n"]}],"source":["# Perplexity for simple models like unigram can blow up easily for large corpora.\n","# And Hence we only evaluate on the first 100 tokens of each data split \n","wiki_train_subsample = \" \".join(wiki_train.split()[:100])\n","wiki_valid_subsample = \" \".join(wiki_valid.split()[:100])\n","wiki_test_subsample = \" \".join(wiki_test.split()[:100])\n","\n","\n","train_ppl = get_unigram_perplexity(wiki_train_subsample, train_unigram_probs)\n","expected_train_ppl = 66.87292185710841\n","print(f\"Training Data Perplexity: {train_ppl}\")\n","print(f\"Expected Training Data Perplexity: {expected_train_ppl}\")\n","assert np.allclose(train_ppl, expected_train_ppl, 1e-4)\n","\n","print(\"**********************************\\n\")\n","\n","valid_ppl = get_unigram_perplexity(wiki_valid_subsample, train_unigram_probs)\n","expected_valid_ppl = 73.3487215828768\n","print(f\"Validation Data Perplexity: {valid_ppl}\")\n","print(f\"Expected Validation Data Perplexity: {expected_valid_ppl}\")\n","assert np.allclose(valid_ppl, expected_valid_ppl, 1e-4)\n","\n","print(\"**********************************\\n\")\n","\n","test_ppl = get_unigram_perplexity(wiki_test_subsample, train_unigram_probs)\n","expected_test_ppl = 54.97601260063733\n","print(f\"Test Data Perplexity: {test_ppl}\")\n","print(f\"Expected Test Data Perplexity: {expected_test_ppl}\")\n","assert np.allclose(test_ppl, expected_test_ppl, 1e-4)\n","\n","print(\"**********************************\\n\")\n"]},{"cell_type":"markdown","id":"2c4cae66","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1a9084fc0d8d2a51d22fc242304f1615","grade":false,"grade_id":"cell-4e9803311447c8f3","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2c4cae66"},"source":["### Task 2.2: Generating texts using N-gram LMs (2.5 Marks)\n","\n","Using the estimated N-gram probabilities we can use the N-gram language models to generate pieces of text. We start with N-1 tokens and then use them as context to predict the next token form our estimated distribution. This process is then done repeatedly till we reach the maximum specified length. Implement the `generate_text_unigram` and `generate_text_ngram` functions below"]},{"cell_type":"code","execution_count":38,"id":"5ee69aa8","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ccefeddf59d10947634e2b831aa966cb","grade":true,"grade_id":"cell-d0ca4be3123b8357","locked":false,"points":1,"schema_version":3,"solution":true,"task":false},"id":"5ee69aa8","executionInfo":{"status":"ok","timestamp":1646947405392,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["def generate_text_unigram(unigram_probs, max_len = 10):\n","    \"\"\"\n","    Generates a random piece of text by sampling from `unigram_probs`\n","    \n","    Inputs:\n","        - unigram_probs (dict): A dictionary containing probabilities of each unigram in the dataset.\n","        - max_len (int) : Maximum length of the sequence (in terms of number of words/tokens) to be generated\n","        \n","    Returns:\n","        - gen_text (str) : Text sampled from the unigram model\n","        \n","    Hint: np.random.choice might come in handy (pay special attention to what to supply as the value of its argument `p`)\n","    \"\"\"\n","    \n","    gen_text = np.random.choice(list(unigram_probs.keys()), max_len, p = list(unigram_probs.values()))\n","    gen_text = ' '.join(gen_text)\n","    \n","    return gen_text"]},{"cell_type":"code","execution_count":39,"id":"bcf5493e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"63850a95c68f862ca2694bee2e1a7a3a","grade":true,"grade_id":"cell-034597726f1551be","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"bcf5493e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646947407543,"user_tz":-330,"elapsed":723,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"1ddbb26c-af0b-409d-9de9-bda851d10928"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: The sustainable 2013 located of of , teeth became International\n","Expected Text: The sustainable 2013 located of of , teeth became International\n"]}],"source":["np.random.seed(42)\n","gen_text = generate_text_unigram(train_unigram_probs, 10)\n","expected_text = 'The sustainable 2013 located of of , teeth became International'\n","print(f\"Generated Text: {gen_text}\")\n","print(f\"Expected Text: {expected_text}\")\n","assert gen_text == expected_text"]},{"cell_type":"code","execution_count":118,"id":"559a34dc","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"deac46a1f988c797d818646a0fa1aa0c","grade":true,"grade_id":"cell-95b05dc181f536f0","locked":false,"points":1.5,"schema_version":3,"solution":true,"task":false},"id":"559a34dc","executionInfo":{"status":"ok","timestamp":1646950209311,"user_tz":-330,"elapsed":400,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":["sentence = ''\n","\n","def generate_text_ngram(ngram_probs, seed_text, max_len = 10):\n","    \"\"\"\n","    Generates a random piece of text by sampling from `ngram_probs`\n","    \n","    Inputs:\n","        - ngram_probs (dict): A dictionary containing containing conditional probabilities for each N-gram in the dataset.\n","        - seed_text (str): A string containing N-1 tokens to use for starting the generated sequence\n","        - max_len (int) : Maximum length of the sequence (in terms of number of words/tokens) to be generated\n","        \n","    Returns:\n","        - gen_text (str) : Text sampled from the ngram model\n","        \n","    Hint: np.random.choice might come in handy (pay special attention to what to supply as the value of its argument `p`)\n","    \n","    Examples:\n","        - Let's say we want to generate text from a bigram model (N = 2). The seed text will always have N-1 tokens i.e.\n","            in this case 1. For the case where seed_text is \"The\", you look up at all bigrams starting with \"The\" in `ngram_probs`.\n","            let's say the sampled word is \"brown\", so next you look at all the bigrams starting with \"brown\" and sample the\n","            next word and repeat the procedure.\n","        - For a trigram model and seed_text \"The brown\", you will similarly start by looking at the trigrams strating with\n","            \"The brown\", sample next word. let's say it is \"fox\", then you look for all the trigrams starting\n","            with \"brown fox\" and sample the next word and so on.\n","    \"\"\"\n","    global sentence\n","    seed = tuple(seed_text.split())\n","    if max_len==0:\n","        sentence = sentence + seed[-1]\n","        return sentence\n","    if sentence == '':\n","        sentence = sentence + seed_text + ' '\n","    else:\n","        sentence = sentence + seed[-1] + ' '\n","    word = np.random.choice(list(ngram_probs[seed].keys()), 1, p = list(ngram_probs[seed].values()))\n","    seed_text = list(seed[1:])\n","    seed_text.append(word[0])\n","    seed_text = ' '.join(seed_text)\n","    return generate_text_ngram(ngram_probs, seed_text, max_len-1)"]},{"cell_type":"code","execution_count":119,"id":"682ff372","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"55ce2c67f03b4d6b519d89a574cf85dd","grade":true,"grade_id":"cell-a1b6099f4b708794","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"682ff372","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646950211834,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"9003300b-e010-447b-edc8-d6b27ae28110"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: The resulting March 1944 . The song , piano at Victoria . Relay events of the United States , was an\n","Expected Text: The resulting March 1944 . The song , piano at Victoria . Relay events of the United States , was an\n"]}],"source":["np.random.seed(42)\n","gen_text = generate_text_ngram(bigram_prob_dist, \"The\", max_len= 20)\n","expected_text = \"The resulting March 1944 . The song , piano at Victoria . Relay events of the United States , was an\"\n","print(f\"Generated Text: {gen_text}\")\n","print(f\"Expected Text: {expected_text}\")\n","assert gen_text == expected_text"]},{"cell_type":"code","execution_count":120,"id":"9bd86b87","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"82a1584a9b693e723d1f24a91cbdc421","grade":true,"grade_id":"cell-e72b515cef45a4bf","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"9bd86b87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646950213702,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"3db0d229-f6d1-42d8-f4ad-f2f98ecc33e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: The resulting scuttling of the strongest aftershock since the <unk> Room . \" Mosley 's 1968 restoration . The bridge was written\n","Expected Text: The resulting scuttling of the strongest aftershock since the <unk> Room . \" Mosley 's 1968 restoration . The bridge was written\n"]}],"source":["sentence = ''\n","\n","np.random.seed(42)\n","gen_text = generate_text_ngram(trigram_prob_dist, \"The resulting\", max_len= 20)\n","expected_text = 'The resulting scuttling of the strongest aftershock since the <unk> Room . \" Mosley \\'s 1968 restoration . The bridge was written'\n","print(f\"Generated Text: {gen_text}\")\n","print(f\"Expected Text: {expected_text}\")\n","assert gen_text == expected_text"]},{"cell_type":"code","execution_count":121,"id":"17b88c4d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5a01cd8e950ba4cd12bb44d9932f2dbb","grade":true,"grade_id":"cell-db6356c48fddf694","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"17b88c4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646950216936,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}},"outputId":"5c9f1aad-5205-4054-81be-6f14a3626466"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: = = = = Liu Kang was designed to produce . By the end of April . The months that receive the most\n","Expected Text: = = = = Liu Kang was designed to produce . By the end of April . The months that receive the most\n"]}],"source":["sentence = ''\n","\n","np.random.seed(42)\n","gen_text = generate_text_ngram(tetragram_prob_dist, \"= = =\", max_len= 20)\n","expected_text = '= = = = Liu Kang was designed to produce . By the end of April . The months that receive the most'\n","print(f\"Generated Text: {gen_text}\")\n","print(f\"Expected Text: {expected_text}\")\n","assert gen_text == expected_text"]},{"cell_type":"code","execution_count":121,"id":"0af41295","metadata":{"id":"0af41295","executionInfo":{"status":"ok","timestamp":1646950221106,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuVf3P7UK5SKwShbVXTiJCKdSuWmMEhXE1esUcltU=s64","userId":"08411958284918760630"}}},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Assignment2a.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}