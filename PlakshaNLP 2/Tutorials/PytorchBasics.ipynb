{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"PytorchBasics.ipynb","provenance":[{"file_id":"13HGy3-uIIy1KD_WFhG4nVrxJC-3nUUkP","timestamp":1645529308166},{"file_id":"1Z6K6nwbb69XfuInMx7igAp-NNVj_2xc3","timestamp":1641358323849},{"file_id":"1k3iiYsFazcBa49j6XASxqqVnLB_mfzDA","timestamp":1611328575574},{"file_id":"1JtwvipJ6mFnOM7HmMLepFR1Gi6m9TJRy","timestamp":1611185498873}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","source":["**Note for Plaksha TLP Students**: This tutorial is a subset of the original CS224N Pytorch tutorial. The full tutorial also contained demonstration of training a Pytorch model on a word window task classification task. To keep things simple and avoid overwhelming you with the details, that part has been stripped away so you can focus on learning the basics first. The concepts covered here should be enough for you to get started with the first assignment. We will cover different NLP use cases step by step in the assignments. However, if interested you can check the full tutorial [here](https://colab.research.google.com/drive/13HGy3-uIIy1KD_WFhG4nVrxJC-3nUUkP?usp=sharing)."],"metadata":{"id":"O3uu2h7aOGhq"}},{"cell_type":"markdown","metadata":{"id":"H6oqGiIXvrMl"},"source":["# CS224N: PyTorch Tutorial (Winter '22)\n","\n","### Author: Dilara Soylu, Ethan Chi\n","\n","In this notebook, we will have a basic introduction to `PyTorch` and work on a toy NLP task. Following resources have been used in preparation of this notebook:\n","* [\"Word Window Classification\" tutorial notebook]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n","* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n","* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n","\n","Many thanks to Angelica Sun and John Hewitt for their feedback.**bold text**"]},{"cell_type":"markdown","source":["# Please make a copy into your own Drive!"],"metadata":{"id":"s2jvK3g_2zrK"}},{"cell_type":"markdown","metadata":{"id":"4gk1UKaNvrMv"},"source":["## Introduction\n","[PyTorch](https://pytorch.org/) is a deep learning framework, one of the two main frameworks alongside [TensorFlow](https://www.tensorflow.org/). \n","Let's start by importing PyTorch:"]},{"cell_type":"code","metadata":{"id":"u0ukr7quvrMx"},"source":["import torch\n","import torch.nn as nn\n","\n","# Import pprint, module we use for making our print statements prettier\n","import pprint\n","pp = pprint.PrettyPrinter()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k10ZRdcBwDP3"},"source":["We are all set to start our tutorial. Let's dive in!"]},{"cell_type":"markdown","metadata":{"id":"OLdSN9ZXvrM0"},"source":["##Part 1: Tensors\n","\n","**Tensors** are PyTorch's most basic building block. Each tensor is a multi-dimensional matrix; for example, a 256x256 square image might be represented by a `3x256x256` tensor, where the first dimension represents color. Here's how to create a tensor:\n"]},{"cell_type":"code","source":["list_of_lists = [\n","  [1, 2, 3],\n","  [4, 5, 6],\n","]\n","print(list_of_lists)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXD7cTLTh4oF","executionInfo":{"status":"ok","timestamp":1642196364243,"user_tz":480,"elapsed":398,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"bed5ab19-a159-49b7-b09c-5d90c7a624ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3], [4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# Initializing a tensor\n","data = torch.tensor([\n","                     [0, 1],    \n","                     [2, 3],\n","                     [4, 5]\n","                    ])\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VLgHhj0n3LM","executionInfo":{"status":"ok","timestamp":1642196368155,"user_tz":480,"elapsed":269,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7c07a288-416d-4d40-cbd5-34d4aba83671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}]},{"cell_type":"markdown","source":["Each tensor has a **data type**: the major data types you'll need to worry about are floats (`torch.float32`) and integers (`torch.int`). You can specify the data type explicitly when you create the tensor:"],"metadata":{"id":"1i7vrR1_oO4I"}},{"cell_type":"code","source":["# Initializing a tensor with an explicit data type\n","# Notice the dots after the numbers, which specify that they're floats\n","data = torch.tensor([\n","                     [0, 1],    \n","                     [2, 3],\n","                     [4, 5]\n","                    ], dtype=torch.float32)\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7nMdqgMoLOa","executionInfo":{"status":"ok","timestamp":1642144103917,"user_tz":480,"elapsed":71,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"ad14ea54-8d7e-4110-e5d5-a19ba48b63d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1.],\n","        [2., 3.],\n","        [4., 5.]])\n"]}]},{"cell_type":"code","source":["# Initializing a tensor with an explicit data type\n","# Notice the dots after the numbers, which specify that they're floats\n","data = torch.tensor([\n","                     [0.11111111, 1],    \n","                     [2, 3],\n","                     [4, 5]\n","                    ], dtype=torch.float32)\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4vr6EYbiOEJ","executionInfo":{"status":"ok","timestamp":1642196434257,"user_tz":480,"elapsed":259,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"c00b1462-e08a-4e44-d6c5-3afe980beb91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1111, 1.0000],\n","        [2.0000, 3.0000],\n","        [4.0000, 5.0000]])\n"]}]},{"cell_type":"code","source":["# Initializing a tensor with an explicit data type\n","# Notice the dots after the numbers, which specify that they're floats\n","data = torch.tensor([\n","                     [0.11111111, 1],    \n","                     [2, 3],\n","                     [4, 5]\n","                    ])\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnB8J23WiUTi","executionInfo":{"status":"ok","timestamp":1642196464481,"user_tz":480,"elapsed":267,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"391d8adf-bb64-4b1e-8088-67d859410878"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1111, 1.0000],\n","        [2.0000, 3.0000],\n","        [4.0000, 5.0000]])\n"]}]},{"cell_type":"markdown","source":["Utility functions also exist to create tensors with given shapes and contents:"],"metadata":{"id":"aiGCmTsrpkP-"}},{"cell_type":"code","source":["zeros = torch.zeros(2, 5)  # a tensor of all zeros\n","print(zeros) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tn-N-z_qYj0","executionInfo":{"status":"ok","timestamp":1642196515753,"user_tz":480,"elapsed":543,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"66649626-4534-4171-c706-0b9ef240e286"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"code","source":["ones = torch.ones(3, 4)   # a tensor of all ones\n","print(ones)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ue26M5Npqoe3","executionInfo":{"status":"ok","timestamp":1642196519983,"user_tz":480,"elapsed":367,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"2be44499-15aa-49da-fe4a-c38be42bec58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n"]}]},{"cell_type":"code","source":["rr = torch.arange(1, 10) # range from [1, 10) \n","print(rr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdvaujtvqokI","executionInfo":{"status":"ok","timestamp":1642196527907,"user_tz":480,"elapsed":468,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"dc6c229a-d171-4fcd-c136-e626b366d82b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}]},{"cell_type":"code","source":["rr + 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lK3q3LRHipHB","executionInfo":{"status":"ok","timestamp":1642196545291,"user_tz":480,"elapsed":329,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"42bf01ee-f811-4c52-a41b-d6a55df71e8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["rr * 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDQ-v6AFiyco","executionInfo":{"status":"ok","timestamp":1642196578658,"user_tz":480,"elapsed":228,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"745bb71f-d87e-4651-ba0b-58a425d9c295"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n","b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)  (3, 4)\n","\n","print(\"A is\", a)\n","print(\"B is\", b)\n","print(\"The product is\", a.matmul(b))\n","print(\"The other product is\", a @ b) # +, -, *, @"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFcjwshvi3pC","executionInfo":{"status":"ok","timestamp":1642196734578,"user_tz":480,"elapsed":434,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"32d104f1-83e1-41e1-9c96-18cc984a0afc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A is tensor([[1, 2],\n","        [2, 3],\n","        [4, 5]])\n","B is tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 8]])\n","The product is tensor([[11, 14, 17, 20],\n","        [17, 22, 27, 32],\n","        [29, 38, 47, 56]])\n","The other product is tensor([[11, 14, 17, 20],\n","        [17, 22, 27, 32],\n","        [29, 38, 47, 56]])\n"]}]},{"cell_type":"code","source":["v = torch.tensor([1, 2, 3])"],"metadata":{"id":"IaE51dx1jitN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["v.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KUR7MGQjmaQ","executionInfo":{"status":"ok","timestamp":1642196794738,"user_tz":480,"elapsed":194,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"6f3c00e6-9bf8-4b19-d003-5f01103f9dcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["torch.tensor([[1, 2, 3], [4, 5, 6]]) @ v  #(2, 3) @ (3)  -> (2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZs43FgLjo9Z","executionInfo":{"status":"ok","timestamp":1642196812330,"user_tz":480,"elapsed":362,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7ad62b85-4949-4fa9-c71b-3cf405cda695"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([14, 32])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["The **shape** of a matrix (which can be accessed by `.shape`) is defined as the dimensions of the matrix. Here's some examples:"],"metadata":{"id":"SCS5z1lip9lq"}},{"cell_type":"code","source":["matr_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print(matr_2d.shape)\n","print(matr_2d)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMptIpZkq0da","executionInfo":{"status":"ok","timestamp":1642196882323,"user_tz":480,"elapsed":191,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"4a1ddcdf-d570-405f-aa16-061a3c6d2354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3])\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n"]}]},{"cell_type":"code","source":["matr_3d = torch.tensor([[[1, 2, 3, 4], [-2, 5, 6, 9]], [[5, 6, 7, 2], [8, 9, 10, 4]], [[-3, 2, 2, 1], [4, 6, 5, 9]]])\n","print(matr_3d)\n","print(matr_3d.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWjD7WjPqC6t","executionInfo":{"status":"ok","timestamp":1642196894049,"user_tz":480,"elapsed":271,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"a69101e2-b0f7-427d-d954-9355dd45ea49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 1,  2,  3,  4],\n","         [-2,  5,  6,  9]],\n","\n","        [[ 5,  6,  7,  2],\n","         [ 8,  9, 10,  4]],\n","\n","        [[-3,  2,  2,  1],\n","         [ 4,  6,  5,  9]]])\n","torch.Size([3, 2, 4])\n"]}]},{"cell_type":"markdown","source":["**Reshaping** tensors can be used to make batch operations easier (more on that later), but be careful that the data is reshaped in the order you expect:"],"metadata":{"id":"7wKqP85rrF-P"}},{"cell_type":"code","source":["rr = torch.arange(1, 16)\n","print(\"The shape is currently\", rr.shape)\n","print(\"The contents are currently\", rr)\n","print()\n","rr = rr.view(5, 3)\n","print(\"After reshaping, the shape is currently\", rr.shape)\n","print(\"The contents are currently\", rr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmUcqHYUrMu1","executionInfo":{"status":"ok","timestamp":1642196925121,"user_tz":480,"elapsed":247,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"47bf5750-4b7a-4877-fded-b7fee6684a30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The shape is currently torch.Size([15])\n","The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n","\n","After reshaping, the shape is currently torch.Size([5, 3])\n","The contents are currently tensor([[ 1,  2,  3],\n","        [ 4,  5,  6],\n","        [ 7,  8,  9],\n","        [10, 11, 12],\n","        [13, 14, 15]])\n"]}]},{"cell_type":"markdown","source":["Finally, you can also inter-convert tensors with **NumPy arrays**:"],"metadata":{"id":"GaykBuhoou3M"}},{"cell_type":"code","source":["import numpy as np\n","\n","# numpy.ndarray --> torch.Tensor:\n","arr = np.array([[1, 0, 5]])\n","data = torch.tensor(arr)\n","print(\"This is a torch.tensor\", data)\n","\n","# torch.Tensor --> numpy.ndarray:\n","new_arr = data.numpy()\n","print(\"This is a np.ndarray\", new_arr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppYiPnlko1Ci","executionInfo":{"status":"ok","timestamp":1642196960247,"user_tz":480,"elapsed":255,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"8bae7875-9287-4667-ca75-2d73c516048a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a torch.tensor tensor([[1, 0, 5]])\n","This is a np.ndarray [[1 0 5]]\n"]}]},{"cell_type":"markdown","source":["One of the reasons why we use **tensors** is *vectorized operations*: operations that be conducted in parallel over a particular dimension of a tensor. "],"metadata":{"id":"hyv1l431q9yA"}},{"cell_type":"code","source":["data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n","print(\"Data is:\", data)\n","\n","# We can perform operations like *sum* over each row...\n","print(\"Taking the sum over columns:\")\n","print(data.sum(dim=0))\n","\n","# or over each column.\n","print(\"Taking thep sum over rows:\")\n","print(data.sum(dim=1))\n","\n","# Other operations are available:\n","print(\"Taking the stdev over rows:\")\n","print(data.std(dim=1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kas2MEFDsJWk","executionInfo":{"status":"ok","timestamp":1642197059215,"user_tz":480,"elapsed":257,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"b653514e-6b94-4daf-8e00-9ca307e6b4a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11., 12., 13., 14.],\n","        [15., 16., 17., 18., 19., 20., 21.],\n","        [22., 23., 24., 25., 26., 27., 28.],\n","        [29., 30., 31., 32., 33., 34., 35.]])\n","Taking the sum over columns:\n","tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n","Taking thep sum over rows:\n","tensor([ 28.,  77., 126., 175., 224.])\n","Taking the stdev over rows:\n","tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"]}]},{"cell_type":"code","source":["data.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPRy-xtuk2tK","executionInfo":{"status":"ok","timestamp":1642197127325,"user_tz":480,"elapsed":277,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"28613828-0196-4f9a-db6b-31178fd18014"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(630.)"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["### Quiz\n","\n","Write code that creates a `torch.tensor` with the following contents:\n","$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n","\n","Now compute the average of each row (`.mean()`) and each column.\n","\n","What's the shape of the results?\n","\n"],"metadata":{"id":"IJ8MjWEMxOVk"}},{"cell_type":"code","source":["data = torch.tensor([[1, 2.2, 9.6], [4, -7.2, 6.3]])\n","\n","row_avg = data.mean(dim=1)\n","col_avg = data.mean(dim=0)\n","\n","print(row_avg.shape)\n","print(row_avg)\n","\n","print(col_avg.shape)\n","print(col_avg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BK0YInGkn3Xy","executionInfo":{"status":"ok","timestamp":1642197430332,"user_tz":480,"elapsed":432,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"10a10cf3-50f4-45fd-8bfb-46c488bb8ba1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2])\n","tensor([4.2667, 1.0333])\n","torch.Size([3])\n","tensor([ 2.5000, -2.5000,  7.9500])\n"]}]},{"cell_type":"markdown","metadata":{"id":"V7BMktFFAkRA"},"source":["**Indexing**\n","\n","You can access arbitrary elements of a tensor using the `[]` operator."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRJN7ovWDsKV","executionInfo":{"status":"ok","timestamp":1642197489760,"user_tz":480,"elapsed":307,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"37e87f97-faca-4a76-981f-5512fa982de2"},"source":["# Initialize an example tensor\n","x = torch.Tensor([\n","                  [[1, 2], [3, 4]],\n","                  [[5, 6], [7, 8]], \n","                  [[9, 10], [11, 12]] \n","                 ])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.,  2.],\n","         [ 3.,  4.]],\n","\n","        [[ 5.,  6.],\n","         [ 7.,  8.]],\n","\n","        [[ 9., 10.],\n","         [11., 12.]]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M67ZiOF1Heyc","executionInfo":{"status":"ok","timestamp":1642197499648,"user_tz":480,"elapsed":554,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"c445e3d3-cd95-44f2-ae17-78f17ba4c895"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2, 2])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guXKE7m8AX1K","executionInfo":{"status":"ok","timestamp":1642197509628,"user_tz":480,"elapsed":335,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"fd53fa4e-ef82-4c71-cc5a-80748df97f9c"},"source":["# Access the 0th element, which is the first row\n","x[0] # Equivalent to x[0, :]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["x[:, 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zn4pW2rkmXuj","executionInfo":{"status":"ok","timestamp":1642197542648,"user_tz":480,"elapsed":461,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"2ae4d716-ba3b-4e1e-df99-cc7a166ae47e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.,  2.],\n","        [ 5.,  6.],\n","        [ 9., 10.]])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["matr = torch.arange(1, 16).view(5, 3)\n","print(matr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvkHJ2pYmgMD","executionInfo":{"status":"ok","timestamp":1642197585853,"user_tz":480,"elapsed":367,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"996b2eed-fa85-4977-efd5-d8bcbe7b9d27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1,  2,  3],\n","        [ 4,  5,  6],\n","        [ 7,  8,  9],\n","        [10, 11, 12],\n","        [13, 14, 15]])\n"]}]},{"cell_type":"code","source":["matr[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXfgmuplmpmg","executionInfo":{"status":"ok","timestamp":1642197607077,"user_tz":480,"elapsed":243,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"cfa54474-e6ea-482d-886e-a67361cba5b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["matr[0, :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rw_qQ9ponJV2","executionInfo":{"status":"ok","timestamp":1642197723046,"user_tz":480,"elapsed":255,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"508bf931-3de1-49fc-f6ce-12bb40646adb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["matr[:, 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw2H2WOcmuyz","executionInfo":{"status":"ok","timestamp":1642197614173,"user_tz":480,"elapsed":252,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"ff455d71-dfd3-4274-a2be-7530cf69f212"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1,  4,  7, 10, 13])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["matr[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9f0q9ZFmysw","executionInfo":{"status":"ok","timestamp":1642197631575,"user_tz":480,"elapsed":258,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7184c4a3-f851-4980-cd6f-8ddf3f9595d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["matr[:, 0:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ucc2UqRVm08h","executionInfo":{"status":"ok","timestamp":1642197639698,"user_tz":480,"elapsed":280,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7b1822ed-77d5-4174-a996-1e40faecafa7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2],\n","        [ 4,  5],\n","        [ 7,  8],\n","        [10, 11],\n","        [13, 14]])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["matr[0:3, 0:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEkhpZiRm24y","executionInfo":{"status":"ok","timestamp":1642197647403,"user_tz":480,"elapsed":244,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"2eba42a0-cce1-406b-c4ef-05279c2d4891"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2],\n","        [4, 5],\n","        [7, 8]])"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["matr[0][2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kN7oaYPdm-zC","executionInfo":{"status":"ok","timestamp":1642197683867,"user_tz":480,"elapsed":204,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"42f54377-9367-4af0-d0ba-bb392d0d69f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["matr[0:3, 2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DYqlPb0ntjV","executionInfo":{"status":"ok","timestamp":1642197896819,"user_tz":480,"elapsed":318,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"99a56d2d-2b8c-4773-d114-a3c28d085ec9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3, 6, 9])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["matr[0:3][2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihEGoU9jn1rf","executionInfo":{"status":"ok","timestamp":1642197904697,"user_tz":480,"elapsed":227,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"77121d65-d19e-40a3-e89c-db019e981307"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([7, 8, 9])"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["matr[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOPG-LeCn369","executionInfo":{"status":"ok","timestamp":1642197913946,"user_tz":480,"elapsed":444,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"5b23be95-6547-4f49-c9c8-7bbf7aa30e66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["matr[[0, 2, 4]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Si9Xnpo-oNcm","executionInfo":{"status":"ok","timestamp":1642198009594,"user_tz":480,"elapsed":245,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"fec4fc4b-9b63-439a-ea38-569b53297318"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2,  3],\n","        [ 7,  8,  9],\n","        [13, 14, 15]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"g8m8EyVvES4-"},"source":["We can also index into multiple dimensions with `:`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Z6GFUcuEL85","executionInfo":{"status":"ok","timestamp":1642144104276,"user_tz":480,"elapsed":69,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"2fb8bfaa-7783-40ad-c09e-6d24e0973f69"},"source":["# Get the top left element of each element in our tensor\n","x[:, 0, 0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 5., 9.])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["x[:, :, :]"],"metadata":{"id":"TRkMhiJNnWAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rm8vc3nuXaEw"},"source":["We can also access arbitrary elements in each dimension. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYhcH9gaWHyW","executionInfo":{"status":"ok","timestamp":1642144104276,"user_tz":480,"elapsed":65,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"ae842696-e0c4-48c5-d5dc-5d4360ef415c"},"source":["# Print x again to see our tensor\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.,  2.],\n","         [ 3.,  4.]],\n","\n","        [[ 5.,  6.],\n","         [ 7.,  8.]],\n","\n","        [[ 9., 10.],\n","         [11., 12.]]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[""],"metadata":{"id":"poKwxYQanqww"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4xl6CW3RrEw","executionInfo":{"status":"ok","timestamp":1642144104277,"user_tz":480,"elapsed":62,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"19f8c35b-ecad-4d3d-e1a3-6b429354eefc"},"source":["# Let's access the 0th and 1st elements, each twice\n","i = torch.tensor([0, 0, 1, 1])\n","x[i]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 2.],\n","         [3., 4.]],\n","\n","        [[1., 2.],\n","         [3., 4.]],\n","\n","        [[5., 6.],\n","         [7., 8.]],\n","\n","        [[5., 6.],\n","         [7., 8.]]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3QYZ8k7Wvqp","executionInfo":{"status":"ok","timestamp":1642144104278,"user_tz":480,"elapsed":59,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"51161c3e-3b2c-4867-b91b-b5a0cac8a4de"},"source":["# Let's access the 0th elements of the 1st and 2nd elements\n","i = torch.tensor([1, 2])\n","j = torch.tensor([0])\n","x[i, j]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5.,  6.],\n","        [ 9., 10.]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"WAELXC--IHS7"},"source":["We can get a `Python` scalar value from a tensor with `item()`. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM-ZujN2IGaQ","executionInfo":{"status":"ok","timestamp":1642144104279,"user_tz":480,"elapsed":56,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"d9691c55-e0f9-49cb-f3b5-ad1e0a6f1d7d"},"source":["x[0, 0, 0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NwxK7d_Ycgs","executionInfo":{"status":"ok","timestamp":1642144104280,"user_tz":480,"elapsed":53,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"877e4c6f-36ee-4986-d47e-a3b86a4f6d33"},"source":["x[0, 0, 0].item()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["### Exercise:\n","\n","Write code that creates a `torch.tensor` with the following contents:\n","$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n","\n","How do you get the first column? The first row?\n","\n"],"metadata":{"id":"bGod5kHa6OOF"}},{"cell_type":"markdown","metadata":{"id":"Re8xiL37eAja"},"source":["## Autograd\n","Pytorch is well-known for its automatic differentiation feature. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oEvBJHWfn8H","executionInfo":{"status":"ok","timestamp":1642198104184,"user_tz":480,"elapsed":400,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"10ad63a8-780b-434e-8d80-0d540b497340"},"source":["# Create an example tensor\n","# requires_grad parameter tells PyTorch to store gradients\n","x = torch.tensor([2.], requires_grad=True)\n","\n","# Print the gradient if it is calculated\n","# Currently None since x is a scalar\n","pp.pprint(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTJazZXkgthP","executionInfo":{"status":"ok","timestamp":1642198107320,"user_tz":480,"elapsed":291,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"63b360ec-8604-4568-8876-c231391143f3"},"source":["# Calculating the gradient of y with respect to x\n","y = x * x * 3 # 3x^2\n","y.backward()\n","pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([12.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"3Hqc2oM3iV6a"},"source":["Let's run backprop from a different tensor again to see what happens."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K--Az0Xiic_z","executionInfo":{"status":"ok","timestamp":1642198151021,"user_tz":480,"elapsed":225,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7d66fa24-a9c0-47d8-dcc9-471ea4b3fddd"},"source":["z = x * x * 3 # 3x^2\n","z.backward()\n","pp.pprint(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([24.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"HhjPkiE6i7ja"},"source":["We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong. "]},{"cell_type":"markdown","metadata":{"id":"pYLWqKIoaOyd"},"source":["## Neural Network Module\n","\n","So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it. "]},{"cell_type":"code","metadata":{"id":"qUmrDpbhV4Tn"},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joGvRWjEbak0"},"source":["### **Linear Layer**\n","We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XfnKI4-a5j9","executionInfo":{"status":"ok","timestamp":1642198234985,"user_tz":480,"elapsed":556,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"90f2307b-3ef6-4d35-f6ee-0971d75c860a"},"source":["# Create the inputs\n","input = torch.ones(2,3,4)\n","# N* H_in -> N*H_out\n","\n","\n","# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n","# dimensional outputs\n","linear = nn.Linear(4, 2)\n","linear_output = linear(input)\n","linear_output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.9886,  0.3687],\n","         [-0.9886,  0.3687],\n","         [-0.9886,  0.3687]],\n","\n","        [[-0.9886,  0.3687],\n","         [-0.9886,  0.3687],\n","         [-0.9886,  0.3687]]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_9XKtAFYpdI","executionInfo":{"status":"ok","timestamp":1642198283035,"user_tz":480,"elapsed":236,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"647dceba-91c2-4be0-8af6-0a5882fe323a"},"source":["list(linear.parameters()) # Ax + b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[-0.3828,  0.1753, -0.3495, -0.0900],\n","         [ 0.2177,  0.3086,  0.3649, -0.4700]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.3418, -0.0526], requires_grad=True)]"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["Data of shape [batch_size, feature_dim] # 4\n","[batch_size, output_dim] # 2\n","\n","linear layer of shape (feature_dim, output_dim)"],"metadata":{"id":"FsyBaqzZppS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAXCCu9keUlW"},"source":["### **Other Module Layers**\n","There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up. "]},{"cell_type":"markdown","metadata":{"id":"yslDOK66fYWn"},"source":["### **Activation Function Layer**\n","We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrJP5CveeOON","executionInfo":{"status":"ok","timestamp":1642198338284,"user_tz":480,"elapsed":303,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"15611d9d-c54d-45cd-bcbf-2e4232e42dd4"},"source":["linear_output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.9886,  0.3687],\n","         [-0.9886,  0.3687],\n","         [-0.9886,  0.3687]],\n","\n","        [[-0.9886,  0.3687],\n","         [-0.9886,  0.3687],\n","         [-0.9886,  0.3687]]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9v5FjQtd4Ck","executionInfo":{"status":"ok","timestamp":1642198349716,"user_tz":480,"elapsed":1239,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"7d09fa49-e1f9-4d14-b669-e64f30962b35"},"source":["sigmoid = nn.Sigmoid()\n","output = sigmoid(linear_output)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.2712, 0.5911],\n","         [0.2712, 0.5911],\n","         [0.2712, 0.5911]],\n","\n","        [[0.2712, 0.5911],\n","         [0.2712, 0.5911],\n","         [0.2712, 0.5911]]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"RiYTthJwhEYT"},"source":["### **Putting the Layers Together**\n","So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtJeOqLxhBLY","executionInfo":{"status":"ok","timestamp":1642144205233,"user_tz":480,"elapsed":150,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"24f295b3-4c73-4960-8455-88ff0adb3032"},"source":["block = nn.Sequential(\n","    nn.Linear(4, 2),\n","    nn.Sigmoid()\n",")\n","\n","input = torch.ones(2,3,4)\n","output = block(input)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.6106, 0.7368],\n","         [0.6106, 0.7368],\n","         [0.6106, 0.7368]],\n","\n","        [[0.6106, 0.7368],\n","         [0.6106, 0.7368],\n","         [0.6106, 0.7368]]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"GkJ81p3GUVPM"},"source":["### Custom Modules\n","\n","Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n","\n","To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n","\n","All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`."]},{"cell_type":"code","metadata":{"id":"J2P7eZiMj32_"},"source":["class MultilayerPerceptron(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size):\n","    # Call to the __init__ function of the super class\n","    super(MultilayerPerceptron, self).__init__()\n","\n","    # Bookkeeping: Saving the initialization parameters\n","    self.input_size = input_size \n","    self.hidden_size = hidden_size \n","\n","    # Defining of our model\n","    # There isn't anything specific about the naming of `self.model`. It could\n","    # be something arbitrary.\n","    self.model = nn.Sequential(\n","        nn.Linear(self.input_size, self.hidden_size),\n","        nn.ReLU(),\n","        nn.Linear(self.hidden_size, self.input_size),\n","        nn.Sigmoid()\n","    )\n","    \n","  def forward(self, x):\n","    output = self.model(x)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2DrfLiBVjNT"},"source":["Here is an alternative way to define the same class. You can see that we can replace `nn.Sequential` by defining the individual layers in the `__init__` method and connecting the in the `forward` method. "]},{"cell_type":"code","metadata":{"id":"9-lqhsqwViIk"},"source":["class MultilayerPerceptron(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size):\n","    # Call to the __init__ function of the super class\n","    super(MultilayerPerceptron, self).__init__()\n","\n","    # Bookkeeping: Saving the initialization parameters\n","    self.input_size = input_size \n","    self.hidden_size = hidden_size \n","\n","    # Defining of our layers\n","    self.linear = nn.Linear(self.input_size, self.hidden_size)\n","    self.relu = nn.ReLU()\n","    self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n","    self.sigmoid = nn.Sigmoid()\n","    \n","  def forward(self, x):\n","    linear = self.linear(x)\n","    relu = self.relu(linear)\n","    linear2 = self.linear2(relu)\n","    output = self.sigmoid(linear2)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQelcFo5bXgU"},"source":["Now that we have defined our class, we can instantiate it and see what it does. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXi0T0FZbV0y","executionInfo":{"status":"ok","timestamp":1642198749399,"user_tz":480,"elapsed":327,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"1f0ae1f6-0dd5-4ba7-f510-487c536f97a0"},"source":["# Make a sample input\n","input = torch.randn(2, 5)\n","\n","# Create our model\n","model = MultilayerPerceptron(5, 3)\n","\n","# Pass our input through our model\n","model(input)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.5962, 0.5057, 0.5963, 0.5812, 0.4767],\n","        [0.5955, 0.5162, 0.5749, 0.5388, 0.5024]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"tCCbjc-Fb2-B"},"source":["We can inspect the parameters of our model with `named_parameters()` and `parameters()` methods. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d23soYIb2WZ","executionInfo":{"status":"ok","timestamp":1642198797350,"user_tz":480,"elapsed":380,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"4d50b5be-8ccf-4dd9-dd7d-078a4b65ff8f"},"source":["list(model.named_parameters())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('linear.weight', Parameter containing:\n","  tensor([[ 0.0368, -0.0646,  0.0542, -0.0379,  0.4065],\n","          [ 0.2702,  0.2839,  0.1291, -0.2588, -0.0564],\n","          [-0.3855,  0.2294, -0.1773,  0.3299,  0.3396]], requires_grad=True)),\n"," ('linear.bias', Parameter containing:\n","  tensor([-0.1430,  0.0027,  0.4313], requires_grad=True)),\n"," ('linear2.weight', Parameter containing:\n","  tensor([[-0.4661,  0.4033, -0.1073],\n","          [-0.5408, -0.4103,  0.0023],\n","          [-0.2667,  0.5137,  0.0963],\n","          [-0.3463, -0.3437,  0.5733],\n","          [ 0.4121, -0.0583, -0.2666]], requires_grad=True)),\n"," ('linear2.bias', Parameter containing:\n","  tensor([0.3213, 0.1311, 0.2186, 0.2109, 0.0190], requires_grad=True))]"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"x5JegycOdMFy"},"source":["## Optimization\n","We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."]},{"cell_type":"code","metadata":{"id":"W0F-TvV0kk-I"},"source":["import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgak6o5dlQWF"},"source":["After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGYFiaT_vXBn","executionInfo":{"status":"ok","timestamp":1642198881766,"user_tz":480,"elapsed":1058,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"8e8042d1-ef9b-459b-f33c-0e2e5bb1632b"},"source":["# Create the y data\n","y = torch.ones(10, 5)\n","\n","# Add some noise to our goal y to generate our x\n","# We want out model to predict our original data, albeit the noise\n","x = y + torch.randn_like(y)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.5254, -0.5026,  0.2936,  1.1698,  0.6667],\n","        [ 1.2823,  0.2541,  1.1641,  0.6377,  1.6982],\n","        [ 0.0122,  0.4521,  0.4833,  1.1832,  0.3251],\n","        [ 2.0469,  0.1789,  0.2834, -0.3078,  1.8006],\n","        [ 0.7365, -0.5649,  0.1238,  0.8654,  1.8826],\n","        [ 2.6088,  3.1178,  2.4707,  2.5342,  1.5063],\n","        [ 0.1240, -0.1938,  0.8643,  1.2870,  2.2391],\n","        [ 1.3622,  3.9153,  1.2991,  0.1062,  1.6210],\n","        [ 0.5091,  1.2953,  1.0652,  1.6663,  1.0086],\n","        [-0.3073, -0.0082,  1.7911,  1.0753,  0.5141]])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"BEsiOdpWvfLj"},"source":["Now, we can define our model, optimizer and the loss function. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oA2XsdsbN8p","executionInfo":{"status":"ok","timestamp":1642198901231,"user_tz":480,"elapsed":424,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"69e4d7c4-d45a-461c-820b-4dd90317e7da"},"source":["# Instantiate the model\n","model = MultilayerPerceptron(5, 3)\n","\n","# Define the optimizer\n","adam = optim.Adam(model.parameters(), lr=1e-1)\n","\n","# Define loss using a predefined loss function\n","loss_function = nn.BCELoss()\n","\n","# Calculate how our model is doing now\n","y_pred = model(x)\n","loss_function(y_pred, y).item()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7129776477813721"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"gtxU7Y8ZufSR"},"source":["Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogl6-Ctmuek6","executionInfo":{"status":"ok","timestamp":1642199015875,"user_tz":480,"elapsed":580,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"202b7ac0-bb0c-44dc-a086-0ccf8b16ebe3"},"source":["# Set the number of epoch, which determines the number of training iterations\n","n_epoch = 10 \n","\n","for epoch in range(n_epoch):\n","  # Set the gradients to 0\n","  adam.zero_grad()\n","\n","  # Get the model predictions\n","  y_pred = model(x)\n","\n","  # Get the loss\n","  loss = loss_function(y_pred, y)\n","\n","  # Print stats\n","  print(f\"Epoch {epoch}: traing loss: {loss}\")\n","\n","  # Compute the gradients\n","  loss.backward()\n","\n","  # Take a step to optimize the weights\n","  adam.step()\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: traing loss: 0.7129776477813721\n","Epoch 1: traing loss: 0.5745089650154114\n","Epoch 2: traing loss: 0.3950759768486023\n","Epoch 3: traing loss: 0.2251298576593399\n","Epoch 4: traing loss: 0.1061786338686943\n","Epoch 5: traing loss: 0.04316363483667374\n","Epoch 6: traing loss: 0.016426226124167442\n","Epoch 7: traing loss: 0.006115884054452181\n","Epoch 8: traing loss: 0.0022449910175055265\n","Epoch 9: traing loss: 0.0008585943141952157\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrMJ8AmqeCY-","executionInfo":{"status":"ok","timestamp":1642199028135,"user_tz":480,"elapsed":452,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"947f78fc-7da9-4676-93b4-93f21761faeb"},"source":["list(model.parameters())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.6363,  0.1566,  1.0603,  0.5428,  1.0728],\n","         [ 0.7450,  0.1028,  0.2315, -0.4288,  0.3089],\n","         [ 0.5470,  1.1053,  0.6960,  0.6960,  0.7097]], requires_grad=True),\n"," Parameter containing:\n"," tensor([1.0350, 0.8359, 0.6403], requires_grad=True),\n"," Parameter containing:\n"," tensor([[0.7256, 0.4603, 1.2903],\n","         [1.4280, 0.7170, 0.3321],\n","         [1.3276, 0.4244, 1.3750],\n","         [1.1533, 0.5717, 1.1169],\n","         [1.0995, 0.2483, 0.8655]], requires_grad=True),\n"," Parameter containing:\n"," tensor([0.3305, 0.4047, 0.9623, 0.5595, 1.1514], requires_grad=True)]"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"4nXApd82wlsF"},"source":["You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRqE7P9EtvuS","executionInfo":{"status":"ok","timestamp":1642199035356,"user_tz":480,"elapsed":1113,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"cf0fb25a-2468-4c41-f184-eda61700918a"},"source":["# See how our model performs on the training data\n","y_pred = model(x)\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.9989, 0.9995, 0.9999, 0.9998, 0.9995],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [0.9969, 0.9956, 0.9997, 0.9988, 0.9983],\n","        [0.9999, 1.0000, 1.0000, 1.0000, 0.9999],\n","        [0.9992, 0.9997, 1.0000, 0.9999, 0.9997],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [0.9999, 0.9999, 1.0000, 1.0000, 1.0000],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [1.0000, 0.9999, 1.0000, 1.0000, 1.0000],\n","        [0.9992, 0.9994, 1.0000, 0.9998, 0.9997]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJng31_Pi2R6","executionInfo":{"status":"ok","timestamp":1642199045078,"user_tz":480,"elapsed":317,"user":{"displayName":"Ethan Chi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17882462692937706002"}},"outputId":"a955b054-b847-4b52-f146-7ed27bc64f0d"},"source":["# Create test data and check how our model performs on it\n","x2 = y + torch.randn_like(y)\n","y_pred = model(x2)\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [0.9979, 0.9996, 0.9999, 0.9996, 0.9992],\n","        [0.9948, 0.9940, 0.9993, 0.9979, 0.9964],\n","        [1.0000, 0.9999, 1.0000, 1.0000, 1.0000],\n","        [0.9982, 0.9874, 0.9998, 0.9989, 0.9983],\n","        [0.9980, 0.9989, 0.9998, 0.9995, 0.9989],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"8WNk6oIZw2xo"},"source":["Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!"]},{"cell_type":"code","source":[""],"metadata":{"id":"iebrU4ZibVIR"},"execution_count":null,"outputs":[]}]}